{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elephant-xyz/photo-meta-data-ai/blob/main/PhotoMedtaData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILCxMc0LIsi-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🐘 Welcome to Step 4 of Elephant Mining\n",
        "\n",
        "Congratulations on reaching **Step 4**! By now, you’ve successfully **minted your County Data Group**. In this notebook, you'll use your **seed data** and **property images** to mint your **Photo Data Group**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What You’ll Do in This Step\n",
        "\n",
        "This notebook allows you to:\n",
        "\n",
        "- Upload your property images  \n",
        "- Mint a new **Photo Data Group**  \n",
        "- Automatically generate a **fact sheet** based on the image metadata  \n",
        "\n",
        "This step completes the visual layer of your dataset, setting you up for further data enrichment.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Prerequisites\n",
        "\n",
        "Before continuing, make sure you’ve completed the following two notebooks:\n",
        "\n",
        "1. [📗 Notebook 1: Seed Minting](https://colab.research.google.com/drive/14tSNSP8Pe-mY4VwX9JhXgfyOvzmN3kC0?usp=chrome_ntp)  \n",
        "2. [📘 Notebook 2: County Data Minting](https://colab.research.google.com/drive/1ZI_eScKFh2kDIZgwXljhOgBIgrenDhRi?usp=chrome_ntp)\n",
        "\n",
        "After running both, you should have the following output files ready:\n",
        "\n",
        "- `upload-results.json`  \n",
        "- `submit.zip`\n",
        "\n",
        "Also ensure you have:\n",
        "\n",
        "- **OpenAI API Key**: Valid API key for AI processing capabilities\n",
        "- **AWS Account with Credentials**: AWS Access Key ID and Secret Access Key for cloud services\n",
        "- **Pinata Account**: JWT token for IPFS storage services\n",
        "- **Elephant Address**: Private key for blockchain integration\n",
        "\n",
        "**Required Environment Variables:**\n",
        "---\n",
        "\n",
        "## 📸 In This Notebook\n",
        "\n",
        "Once your image files are uploaded:\n",
        "\n",
        "1. The images will be minted into the **Photo Data Group**  \n",
        "2. A **fact sheet** will be generated for inspection  \n",
        "3. You can continue with **image-based metadata extraction**  \n",
        "4. This will lead to a complete and enriched data product  \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mz5mj3mc9A9r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Lh8z_QIuBx"
      },
      "source": [
        "## 📥 Step 1: Upload the `.env` File\n",
        "\n",
        "This notebook requires a `.env` file containing your API keys and credentials. Create a file with the following environment variables:\n",
        "\n",
        "| Variable Name | Purpose |\n",
        "|---|---|\n",
        "| `OPENAI_API_KEY` | Access to OpenAI API |\n",
        "| `AWS_ACCESS_KEY_ID` | AWS access key |\n",
        "| `AWS_SECRET_ACCESS_KEY` | AWS secret access key |\n",
        "| `S3_BUCKET_NAME` | Your S3 bucket name |\n",
        "| `IMAGES_DIR` | Directory path for images |\n",
        "| `ELEPHANT_PRIVATE_KEY` | Elephant wallet private key |\n",
        "| `PINATA_JWT` | Pinata authentication token |\n",
        "\n",
        "### To upload:\n",
        "1. Click the **folder icon** 📂 in the left sidebar\n",
        "2. Click the **\"Upload\"** button\n",
        "3. Select your `.env` file\n",
        "\n",
        "### Example `.env` file:\n",
        "```env\n",
        "OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "AWS_ACCESS_KEY_ID=XXXXXX\n",
        "AWS_SECRET_ACCESS_KEY=XXXXXX\n",
        "S3_BUCKET_NAME=your-s3-bucket-name-here\n",
        "IMAGES_DIR=images\n",
        "ELEPHANT_PRIVATE_KEY=xxxxx\n",
        "PINATA_JWT=xxxxx\n",
        "```\n",
        "\n",
        "> ⚠️ **Security Note:** Never commit your `.env` file to version control or share it publicly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A8fm24ILQ8s"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q36E40RNVE3"
      },
      "source": [
        "## Step 2: Upload `upload_results.csv`\n",
        "\n",
        "Upload the `upload_results.csv` file to the `/content/` directory.\n",
        "\n",
        "> 📌 **Important**: This file was generated by running **Step 2** of the [Seed Data Notebook](https://colab.research.google.com/drive/14tSNSP8Pe-mY4VwX9JhXgfyOvzmN3kC0?usp=sharing#scrollTo=OFKp4E49651Z)\n",
        "\n",
        "The file should now be downloaded and ready to upload to `/content/upload_results.csv`\n",
        "\n",
        "## Step 3: Upload `submit.zip`\n",
        "\n",
        "Upload the `submit.zip` file to the `/content/` directory.\n",
        "\n",
        "> 📌 **Important**: This file was generated by running **Step 3** of the [County Data Notebook](https://colab.research.google.com/drive/1ZI_eScKFh2kDIZgwXljhOgBIgrenDhRi#scrollTo=HA0ppLFpUm1j)\n",
        "\n",
        "The file should now be downloaded and ready to upload to `/content/submit.zip`\n",
        "\n",
        "## Step 4: Verify Data Exists\n",
        "\n",
        "Once both files are uploaded to `/content/`, you can proceed with the main workflow that depends on these generated datasets.\n",
        "\n",
        "**Expected file locations:**\n",
        "- `/content/upload-results.csv`\n",
        "- `/content/submit.zip`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content/upload-results.csv\n",
        "!ls -la /content/submit.zip"
      ],
      "metadata": {
        "id": "DdykRvx-3jW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F000A7O3PtrJ"
      },
      "source": [
        "##Step 5: Install Package & Setup Folders\n",
        "This step:\n",
        "\n",
        "Installs the photo-meta-data-ai package from GitHub\n",
        "Creates all necessary folders for the project\n",
        "Saves installation details to a log file for troubleshooting\n",
        "\n",
        "Takes 1-2 minutes to complete. Once finished, you'll have the AI package installed and folder structure ready for processing photos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yiY6rgFYPvU6"
      },
      "outputs": [],
      "source": [
        "# 1. Install the package\n",
        "!pip install --force-reinstall --no-cache-dir git+https://github.com/elephant-xyz/photo-meta-data-ai.git > /content/install_log.txt 2>&1\n",
        "\n",
        "# 2. Set up folders\n",
        "!colab-folder-setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojt2s4z6NxC1"
      },
      "source": [
        "##Step 6: Upload Images to Property Subfolders\n",
        "Upload your property images into the pre-created subfolders:\n",
        "\n",
        "Each property already has a subfolder named with its Parcel ID under the images folder\n",
        "Simply drag and drop your images into the correct property subfolder\n",
        "All images for a specific property should go in that property's designated folder\n",
        "\n",
        "The AI will process each property's images and generate metadata organized by Parcel ID.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 and 8: Placeholder Photo Data Group minting"
      ],
      "metadata": {
        "id": "fX_uvmsi1wro"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FL2oPh-S3eya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJo3i5enWo0J"
      },
      "source": [
        "## Step 9: Setup and Run AWS Rekognition\n",
        "\n",
        "The system automatically sets up and runs Amazon Rekognition to analyze your property images:\n",
        "\n",
        "- Connects to AWS Rekognition service for AI-powered image analysis\n",
        "- Processes all images in your property folders automatically\n",
        "- Extracts detailed information like room types, architectural features, and property characteristics\n",
        "\n",
        "No action needed from you - the system handles everything automatically and will notify you when processing is complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G2eZfzTpOGV0"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall --no-cache-dir git+https://github.com/elephant-xyz/photo-meta-data-ai.git > /content/install_log.txt 2>&1\n",
        "!bucket-manager\n",
        "!unzip-county-data\n",
        "!upload-to-s3\n",
        "!photo-categorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo7Jjafq6N1i"
      },
      "source": [
        "## Step 10: Running AI to Extract Data from Images\n",
        "\n",
        "The AI system now analyzes your property images to extract valuable metadata:\n",
        "\n",
        "1. **Image Analysis**: AI examines each photo to identify rooms, features, and property details\n",
        "2. **Data Extraction**: System pulls out structured information like room types, square footage estimates, architectural elements, and condition assessments\n",
        "\n",
        "The process runs automatically across all your uploaded property images, generating comprehensive metadata reports for each parcel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "67Sqqv2xcurU"
      },
      "outputs": [],
      "source": [
        "!ai-analyzer --local-folders --parallel-categories --all-properties\n",
        "!property-summarizer --all-properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WEPamAD7h4F"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKHWrLc47vUv"
      },
      "source": [
        "## Step 11: Data Validation and Submission\n",
        "\n",
        "The system validates extracted data and prepares it for final submission:\n",
        "\n",
        "1. **Data Validation**: Reviews and verifies all extracted metadata for accuracy\n",
        "2. **Submission Preparation**: Validated data is formatted and organized for CLI submission\n",
        "3. **CLI Submission**: System automatically submits the processed data through the command line interface\n",
        "4. **Fact Sheet Generation**: Creates comprehensive property fact sheets with all extracted information, images, and metadata\n",
        "\n",
        "Final deliverables include validated property reports and detailed fact sheets ready for use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Z33zRmVR7zVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b12673d-0239-4f90-c3d7-c2ed89db266d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1395, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 325, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "                              ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 286, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fix-schema-validation\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/src/fix_schema_validation.py\", line 625, in main\n",
            "    process_directory(output_dir)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/src/fix_schema_validation.py\", line 599, in process_directory\n",
            "    schemas = load_all_schemas()\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/src/fix_schema_validation.py\", line 76, in load_all_schemas\n",
            "    schema = fetch_schema_from_ipfs(cid)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/src/fix_schema_validation.py\", line 57, in fetch_schema_from_ipfs\n",
            "    response = requests.get(url, timeout=60)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 565, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1411, in getresponse\n",
            "    response.close()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 424, in close\n",
            "    def close(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!fix-schema-validation\n",
        "!copy-all-data-for-submission\n",
        "!copy-all-files-from-zip\n",
        "!npx @elephant-xyz/cli@latest validate-and-upload submit-photo --output-csv submit-results.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf submit-photo"
      ],
      "metadata": {
        "id": "WeOLWGQTueT2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Submitting Your Data to the Blockchain\n",
        "\n",
        "### Submitting Your Data\n",
        "\n",
        "After running the upload command in the notebook:\n",
        "\n",
        "1. **Download your results file**\n",
        "   - The notebook will generate `submit-results.csv`\n",
        "   - This file contains your data hashes and IPFS CIDs\n",
        "   - Download it to your computer\n",
        "\n",
        "2. **Visit the Oracle Submission Portal**\n",
        "   - Go to https://oracle.elephant.xyz/\n",
        "   - Connect your MetaMask wallet when prompted\n",
        "   - Upload your `submit-results.csv` file\n",
        "\n",
        "3. **Submit transactions**\n",
        "   - The portal will read your CSV and prepare transactions\n",
        "   - Click \"Submit to Contract\" to begin\n",
        "   - MetaMask will pop up for each data entry\n",
        "   - Confirm each transaction (small gas fee applies)\n",
        "   - Wait for confirmations between submissions\n",
        "\n",
        "Once complete, your data is permanently recorded on the blockchain. You'll receive vMahout tokens as rewards after consensus is reached (when 3 different oracles submit matching data hashes)."
      ],
      "metadata": {
        "id": "-DsIg-MR3Rrj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R8Lh8z_QIuBx",
        "6Q36E40RNVE3",
        "F000A7O3PtrJ",
        "fX_uvmsi1wro",
        "xJo3i5enWo0J",
        "uo7Jjafq6N1i",
        "XKHWrLc47vUv"
      ],
      "authorship_tag": "ABX9TyMlof/S8jv8zwXsCCCkwn2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}